{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "from Bio import Entrez\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_from_doi(pm_id):\n",
    "    Entrez.email = \"ae22b024@smail.iitm.ac.in\"\n",
    "    try:\n",
    "        handle = Entrez.efetch(db=\"pubmed\", id=pm_id, rettype=\"full\", retmode=\"xml\")\n",
    "        xml_data = handle.read()\n",
    "        handle.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for PM ID {pm_id}: {e}\")\n",
    "        return False\n",
    "    \n",
    "    root = ET.fromstring(xml_data)\n",
    "\n",
    "    def language_check():\n",
    "        language_tag = root.find('.//Language')\n",
    "        if language_tag is not None and language_tag.text is not None:\n",
    "            if language_tag.text == \"eng\":\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        return True    # might become a problem in future\n",
    "    \n",
    "    doi_found = None\n",
    "    for eloc in root.findall('.//ELocationID'):\n",
    "        if eloc.get('EIdType') == 'doi':\n",
    "            doi_found = eloc.text.strip()\n",
    "            break\n",
    "    \n",
    "    if doi_found is None:\n",
    "        for article_id in root.findall('.//ArticleId'):\n",
    "            if article_id.get('IdType') == 'doi':\n",
    "                doi_found = article_id.text.strip()\n",
    "                break\n",
    "    \n",
    "    if language_check:\n",
    "        return doi_found\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "https://www.pnas.org/doi/abs/10.1073/pnas.2311241121?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub  0pubmed 38838020\n"
     ]
    }
   ],
   "source": [
    "# Extracting link from the bottom of the page (Also can we conclude if there is link on the bottom of the page then it's there on the side also)\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def language_check_for_bottom_link(pm_id):\n",
    "    Entrez.email = \"ae22b024@smail.iitm.ac.in\"\n",
    "    try:\n",
    "        handle = Entrez.efetch(db=\"pubmed\", id=pm_id, rettype=\"full\", retmode=\"xml\")\n",
    "        xml_data = handle.read()\n",
    "        handle.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for PM ID {pm_id}: {e}\")\n",
    "        return False\n",
    "    \n",
    "    root = ET.fromstring(xml_data)\n",
    "\n",
    "    language_tag = root.find('.//Language')\n",
    "    if language_tag is not None and language_tag.text is not None:\n",
    "        if language_tag.text == \"eng\":\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    return True    # might become a problem in future\n",
    "\n",
    "\n",
    "def extract_full_text_url(url, pm_id):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        div_element = soup.find('div', class_='full-text-links-list')\n",
    "\n",
    "        if div_element != None:\n",
    "          anchor_element = div_element.find('a', class_='link-item')\n",
    "          href = anchor_element.get('href')\n",
    "          if language_check_for_bottom_link(pm_id):\n",
    "              print(href, pm_id)\n",
    "              return href\n",
    "          else:\n",
    "              print(\"None\")\n",
    "              return None\n",
    "          \n",
    "        else:\n",
    "          print(\"None\")\n",
    "          x = fetch_from_doi(pm_id)\n",
    "          url = f\"https://doi.org/{x}\"\n",
    "          return url\n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
    "        x = fetch_from_doi(pm_id)\n",
    "        url = f\"https://doi.org/{x}\"\n",
    "        return url\n",
    "\n",
    "\n",
    "#l = [38801081, 38830938, 38801746, 38800010]\n",
    "#l = [38848579]   # the link redirects to pdf, doi \n",
    "#l = [38848750]   # everything in German \n",
    "#l = [38857300, 38843221, 37967080, 38011087] # PLOS ONE\n",
    "l = [38861610, 38838020]\n",
    "full_text_url = []\n",
    "for i in l:\n",
    "  full_text_url.append(extract_full_text_url('https://pubmed.ncbi.nlm.nih.gov/' + f'{i}/', i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "Table saved.\n",
      "Text extracted.\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "Table saved.\n",
      "Text extracted.\n"
     ]
    }
   ],
   "source": [
    "def get_full_text_table(full_url, j):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(full_url)\n",
    "\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    text_elements = []\n",
    "\n",
    "    # Extract text from <div> elements with role=\"paragraph\" and header tags\n",
    "    for element in soup.find_all(['div', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "        if element.name == 'div' and element.get('role') == 'paragraph':\n",
    "            text_elements.append(element.get_text())\n",
    "        elif element.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:\n",
    "            text_elements.append(element.get_text())\n",
    "\n",
    "    # Combine all text elements into a single string\n",
    "    full_text = ' '.join(text_elements)\n",
    "\n",
    "    # Extract and print tables\n",
    "    tables = soup.find_all('table')\n",
    "    for table in tables:\n",
    "        headers = [th.text.strip() for th in table.find_all('th')]\n",
    "        rows = []\n",
    "        for tr in table.find_all('tr'):\n",
    "            cells = tr.find_all('td')\n",
    "            if not cells:\n",
    "                continue\n",
    "            row = [cell.text.strip() for cell in cells]\n",
    "            rows.append(row)\n",
    "\n",
    "        df = pd.DataFrame(rows, columns=headers if headers else None)\n",
    "        print(df)\n",
    "        print(\"Table saved.\")\n",
    "\n",
    "    print(\"Text extracted.\")\n",
    "    with open(f'extracted_text_{j}.txt', 'w', encoding='utf-8') as file:\n",
    "        file.write(full_text)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "for j, url in enumerate(full_text_url, start=1):\n",
    "    if url:\n",
    "        get_full_text_table(url, j)\n",
    "    else:\n",
    "        print(\"can't extract: maybe not written in english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def get_full_text_table(full_url, j):\\n    driver = webdriver.Chrome()\\n    driver.get(full_url)\\n\\n    html = driver.page_source\\n    soup = BeautifulSoup(html, \\'html.parser\\')\\n\\n    p_tags = soup.find_all([\\'p\\', \\'h1\\', \\'h2\\', \\'h3\\', \\'h4\\', \\'h5\\', \\'h6\\'])\\n    text = \\' \\'.join([p.get_text() for p in p_tags])\\n\\n    tables = soup.find_all(\\'table\\')\\n    for table in tables:\\n        headers = [th.text.strip() for th in table.find_all(\\'th\\')]\\n        rows = []\\n        for tr in table.find_all(\\'tr\\'):\\n            cells = tr.find_all(\\'td\\')\\n            if not cells:\\n                continue\\n            row = [cell.text.strip() for cell in cells]\\n            rows.append(row)\\n\\n        df = pd.DataFrame(rows, columns=headers)\\n        print(df)\\n        print(\"Table saved.\")\\n\\n    print(\"Text extracted.\")\\n    #print(text)\\n    with open(f\\'extracted_text_{j}.txt\\', \\'w\\', encoding=\\'utf-8\\') as file:\\n        #file.write(\"\\n\\n\\n\\n\")\\n        file.write(text)\\n\\n    driver.quit()\\n\\nfor j, url in enumerate(full_text_url, start=1):\\n    if url:\\n        get_full_text_table(url, j)\\n    else:\\n        print(\"can\\'t extract: maybe not written in english\")'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def get_full_text_table(full_url, j):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(full_url)\n",
    "\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    p_tags = soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "    text = ' '.join([p.get_text() for p in p_tags])\n",
    "\n",
    "    tables = soup.find_all('table')\n",
    "    for table in tables:\n",
    "        headers = [th.text.strip() for th in table.find_all('th')]\n",
    "        rows = []\n",
    "        for tr in table.find_all('tr'):\n",
    "            cells = tr.find_all('td')\n",
    "            if not cells:\n",
    "                continue\n",
    "            row = [cell.text.strip() for cell in cells]\n",
    "            rows.append(row)\n",
    "\n",
    "        df = pd.DataFrame(rows, columns=headers)\n",
    "        print(df)\n",
    "        print(\"Table saved.\")\n",
    "\n",
    "    print(\"Text extracted.\")\n",
    "    #print(text)\n",
    "    with open(f'extracted_text_{j}.txt', 'w', encoding='utf-8') as file:\n",
    "        #file.write(\"\\n\\n\\n\\n\")\n",
    "        file.write(text)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "for j, url in enumerate(full_text_url, start=1):\n",
    "    if url:\n",
    "        get_full_text_table(url, j)\n",
    "    else:\n",
    "        print(\"can't extract: maybe not written in english\")'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
